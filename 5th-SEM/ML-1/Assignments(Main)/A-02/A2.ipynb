{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edb3a79c-da8f-4a27-bcde-f06f0e65bb1f",
   "metadata": {},
   "source": [
    "## Major Assignment – 2 (Chapter 3 – 4)\n",
    "### Part B – Lab Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286b9949-35cd-4866-8434-b9f52c432f4a",
   "metadata": {},
   "source": [
    "Q1. An automobile company wants to predict a car’s mpg value from its physical attributes.  \n",
    "Tasks:  \n",
    "(a) Load the dataset auto_mpg.csv and remove missing values.  \n",
    "(b) Identify predictor and target variables.  \n",
    "(c) Perform data splitting (80% train, 20% test).  \n",
    "(d) Fit a Linear Regression model and predict test outcomes.  \n",
    "(e) Evaluate the model using Mean Squared Error and R² score.  \n",
    "(f) Discuss: If the R² score = 0.85, what does it imply about model performance?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f02356a-c08d-413d-8e2e-d8b5b516a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# (a) Load dataset and remove missing values\n",
    "df = pd.read_csv(\"auto_mpg.csv\")\n",
    "df = df.dropna()\n",
    "\n",
    "# (b) Predictor and target variables\n",
    "X = df[['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration']]\n",
    "y = df['mpg']\n",
    "\n",
    "# (c) Train-test split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# (d) Fit Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict test outcomes\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# (e) Evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R²:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3d415-6567-4f97-b707-f2f4cc2930d5",
   "metadata": {},
   "source": [
    "Q2. Exploring random sampling methods to estimate model uncertainty.  \n",
    "Tasks:  \n",
    "(a) From the btissue.csv data, extract only the feature columns (excluding labels).  \n",
    "(b) Using the resample() method, create a bootstrap sample of 100 observations.  \n",
    "(c) Show the first 10 rows of the sample and identify if any rows are repeated.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bd2837-229b-43f9-86c6-63a11dbf0642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# (a) Load data and extract feature columns (remove label column)\n",
    "df = pd.read_csv(\"btissue.csv\")\n",
    "X = df.drop('label', axis=1)\n",
    "\n",
    "# (b) Create a bootstrap sample of 100 observations\n",
    "boot_sample = resample(X, n_samples=100, replace=True, random_state=42)\n",
    "\n",
    "# (c) Show first 10 rows and check repeated rows\n",
    "print(boot_sample.head(10))\n",
    "\n",
    "# Identify repeated rows\n",
    "duplicates = boot_sample[boot_sample.duplicated()]\n",
    "print(\"Repeated rows:\")\n",
    "print(duplicates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53402ec-f43f-44cb-9f2d-478a7897d6cf",
   "metadata": {},
   "source": [
    "Q3. Instead of relying on a single train–test split, you want to check how consistent your model is.  \n",
    "Tasks:  \n",
    "(a) Using the btissue.csv dataset, implement 5-fold cross-validation.  \n",
    "(b) For each fold, print the train/test indices and record how many samples are used for training vs testing.  \n",
    "(c) Visualize or summarize how different folds cover the entire dataset without overlap  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af81819-82e2-4f75-9382-3ac7dd87f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# (a) Load dataset\n",
    "df = pd.read_csv(\"btissue.csv\")\n",
    "\n",
    "# Assume last column is the label\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_num = 1\n",
    "all_train_sizes = []\n",
    "all_test_sizes = []\n",
    "\n",
    "# (b) Print train/test indices and sample counts\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(f\"Fold {fold_num}\")\n",
    "    print(\"Train indices:\", train_index)\n",
    "    print(\"Test indices :\", test_index)\n",
    "    print(\"Train size:\", len(train_index))\n",
    "    print(\"Test size :\", len(test_index))\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    all_train_sizes.append(len(train_index))\n",
    "    all_test_sizes.append(len(test_index))\n",
    "    \n",
    "    fold_num += 1\n",
    "\n",
    "# (c) Summary of coverage across folds\n",
    "print(\"Train sizes for each fold:\", all_train_sizes)\n",
    "print(\"Test sizes for each fold:\", all_test_sizes)\n",
    "print(\"\\nEach data point appears in exactly one test fold, ensuring full non-overlapping coverage.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1540bf2-ad73-4b8a-ae4e-d1c1ffd6116e",
   "metadata": {},
   "source": [
    "Q4. Testing two validation techniques to measure model generalization.  \n",
    "Tasks:  \n",
    "(a) Use the btissue.csv dataset and a Decision Tree Classifier.  \n",
    "(b) Evaluate model performance using:  \n",
    "i) Holdout (80/20 split)  \n",
    "ii) 5-Fold Cross-Validation  \n",
    "(c) Compare the accuracy results from both methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554d4883-bac0-4710-a99a-b94efc8811ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# (a) Load dataset\n",
    "df = pd.read_csv(\"btissue.csv\")\n",
    "\n",
    "# Assume last column is the label\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# -------------------------------\n",
    "# (b.i) Holdout Validation (80/20)\n",
    "# -------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_holdout = DecisionTreeClassifier(random_state=42)\n",
    "model_holdout.fit(X_train, y_train)\n",
    "y_pred = model_holdout.predict(X_test)\n",
    "\n",
    "holdout_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Holdout Accuracy:\", holdout_accuracy)\n",
    "\n",
    "# -------------------------------\n",
    "# (b.ii) 5-Fold Cross-Validation\n",
    "# -------------------------------\n",
    "model_cv = DecisionTreeClassifier(random_state=42)\n",
    "cv_scores = cross_val_score(model_cv, X, y, cv=5)\n",
    "\n",
    "print(\"Cross-Validation Accuracies:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "\n",
    "# -------------------------------\n",
    "# (c) Comparison\n",
    "# -------------------------------\n",
    "print(\"\\nComparison:\")\n",
    "print(\"Holdout Accuracy:\", holdout_accuracy)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b3f2f1-7137-4a5d-bf8c-060485120dd8",
   "metadata": {},
   "source": [
    "Q5. Feature Creation from Structured Data  \n",
    "(a) Using a dataset containing columns like Age, Income, and Spending Score, construct  \n",
    "new derived features such as Age Group, Income-to-Spending Ratio, and Normalized Spending.  \n",
    "(b) Plot and analyze how the new features correlate with the target variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1f5798-d295-4e45-92e0-10ed4543f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"data.csv\")   # contains Age, Income, SpendingScore, Target\n",
    "\n",
    "# -----------------------------\n",
    "# (a) Create new features\n",
    "# -----------------------------\n",
    "\n",
    "# Age Group\n",
    "df['AgeGroup'] = pd.cut(df['Age'],\n",
    "                        bins=[0, 25, 45, 65, 100],\n",
    "                        labels=['Young', 'Adult', 'Middle', 'Senior'])\n",
    "\n",
    "# Income-to-Spending Ratio\n",
    "df['Income_Spend_Ratio'] = df['Income'] / df['SpendingScore']\n",
    "\n",
    "# Normalized Spending\n",
    "scaler = MinMaxScaler()\n",
    "df['Normalized_Spending'] = scaler.fit_transform(df[['SpendingScore']])\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# -----------------------------\n",
    "# (b) Correlation analysis\n",
    "# -----------------------------\n",
    "\n",
    "# Convert AgeGroup to numbers for correlation\n",
    "df['AgeGroup_num'] = df['AgeGroup'].cat.codes\n",
    "\n",
    "# Plot each new feature vs target\n",
    "plt.scatter(df['AgeGroup_num'], df['Target'])\n",
    "plt.xlabel(\"Age Group (encoded)\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.title(\"Age Group vs Target\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df['Income_Spend_Ratio'], df['Target'])\n",
    "plt.xlabel(\"Income/Spending Ratio\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.title(\"Income-to-Spending Ratio vs Target\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df['Normalized_Spending'], df['Target'])\n",
    "plt.xlabel(\"Normalized Spending\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.title(\"Normalized Spending vs Target\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bed79c-b5ae-4975-9bc1-a2f269703ef6",
   "metadata": {},
   "source": [
    "Q6. Load the Iris dataset and select a subset of features manually using the .iloc function.  \n",
    "Train a simple Decision Tree Classifier using only the selected subset of features and  \n",
    "compare its performance with the model trained using all features.  \n",
    "Tasks:  \n",
    "(a) Load the Iris dataset from sklearn.datasets.  \n",
    "(b) Create a DataFrame and display the first few rows.  \n",
    "(c) Train a Decision Tree Classifier using all features and record the accuracy.  \n",
    "(d) Select a subset of columns (for example, the first two features: sepal length and sepal width) using .iloc.  \n",
    "(e) Train another model using only the selected features and evaluate its accuracy.  \n",
    "(f) Compare and discuss the results of both models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ccfc97-b0d8-4ed6-8d9e-f6a0c4b83fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# (a) Load Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# (b) Create DataFrame\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "print(df.head())\n",
    "\n",
    "# (c) Train model using ALL features\n",
    "X_all = df.iloc[:, :-1]\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_all = DecisionTreeClassifier(random_state=42)\n",
    "model_all.fit(X_train, y_train)\n",
    "y_pred_all = model_all.predict(X_test)\n",
    "\n",
    "accuracy_all = accuracy_score(y_test, y_pred_all)\n",
    "print(\"Accuracy using all features:\", accuracy_all)\n",
    "\n",
    "# (d) Select subset of features (first two columns)\n",
    "X_subset = df.iloc[:, :2]\n",
    "\n",
    "# (e) Train model using subset\n",
    "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(\n",
    "    X_subset, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_subset = DecisionTreeClassifier(random_state=42)\n",
    "model_subset.fit(X_train_s, y_train_s)\n",
    "y_pred_subset = model_subset.predict(X_test_s)\n",
    "\n",
    "accuracy_subset = accuracy_score(y_test_s, y_pred_subset)\n",
    "print(\"Accuracy using subset features:\", accuracy_subset)\n",
    "\n",
    "# (f) Comparison\n",
    "print(\"\\nComparison:\")\n",
    "print(\"All features accuracy   :\", accuracy_all)\n",
    "print(\"Subset features accuracy:\", accuracy_subset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee146368-3c39-4734-827a-f3fca197f312",
   "metadata": {},
   "source": [
    "Q7. Load the Iris dataset and apply Principal Component Analysis (PCA) to reduce its four  \n",
    "numerical features (sepal length, sepal width, petal length, petal width) into two principal  \n",
    "components. Visualize the transformed data in a 2D scatter plot to observe how the classes  \n",
    "(Setosa, Versicolor, Virginica) are separated in the reduced feature space. Additionally,  \n",
    "display the explained variance ratio for each component.  \n",
    "Tasks:  \n",
    "(a) Load the Iris dataset using sklearn.datasets.  \n",
    "(b) Perform PCA to reduce the dataset to two components.  \n",
    "(c) Create a new DataFrame containing the two principal components and target labels.  \n",
    "(d) Plot the two components using a scatter plot with different colors for each class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b63f50-3ae8-4c4d-888b-e6550f34458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# (a) Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# (b) PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Explained variance ratio\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# (c) New DataFrame with components + labels\n",
    "df_pca = pd.DataFrame({\n",
    "    \"PC1\": X_pca[:, 0],\n",
    "    \"PC2\": X_pca[:, 1],\n",
    "    \"target\": y\n",
    "})\n",
    "print(df_pca.head())\n",
    "\n",
    "# (d) 2D scatter plot\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "for class_value, class_name in enumerate(iris.target_names):\n",
    "    plt.scatter(\n",
    "        df_pca[df_pca.target == class_value][\"PC1\"],\n",
    "        df_pca[df_pca.target == class_value][\"PC2\"],\n",
    "        label=class_name\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"PCA (2 Components) - Iris Dataset\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c28a337-9180-46c9-94be-4b8ada4cf8a3",
   "metadata": {},
   "source": [
    "Q8. Create a dataset containing employee information, including Department, Job Role, and  \n",
    "Marital Status. Convert all categorical columns into numeric form so that the dataset can be  \n",
    "used effectively for training machine learning models. Use appropriate encoding techniques  \n",
    "such as Label Encoding and One-Hot Encoding.  \n",
    "Tasks:  \n",
    "(a) Create a DataFrame with the following columns and sample data:  \n",
    "Department (e.g., HR, IT, Finance)  \n",
    "Job_Role (e.g., Manager, Analyst, Clerk)  \n",
    "Marital_Status (e.g., Single, Married, Divorced)  \n",
    "(b) Display the original dataset.  \n",
    "(c) Encode categorical columns using:  \n",
    "Label Encoding for ordered or binary categories.  \n",
    "One-Hot Encoding for nominal categories  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95778727-0217-4b3a-aad7-ca183c2e265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# (a) Create sample DataFrame\n",
    "data = {\n",
    "    \"Department\": [\"HR\", \"IT\", \"Finance\", \"IT\", \"HR\"],\n",
    "    \"Job_Role\": [\"Manager\", \"Analyst\", \"Clerk\", \"Manager\", \"Clerk\"],\n",
    "    \"Marital_Status\": [\"Single\", \"Married\", \"Divorced\", \"Married\", \"Single\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# (b) Display original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# (c) Encoding\n",
    "\n",
    "# Label Encoding for Marital_Status (can be considered ordered/binary-like)\n",
    "le = LabelEncoder()\n",
    "df[\"Marital_Status_Encoded\"] = le.fit_transform(df[\"Marital_Status\"])\n",
    "\n",
    "# One-Hot Encoding for nominal columns\n",
    "df_encoded = pd.get_dummies(df, columns=[\"Department\", \"Job_Role\"])\n",
    "\n",
    "print(\"\\nEncoded Dataset:\")\n",
    "print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca5025c-a85e-4160-8828-c9d9a755b720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
